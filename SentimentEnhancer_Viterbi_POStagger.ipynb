{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "8ca0d700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\VENUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the necessary packages and donwload treebank\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import treebank\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "79403718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Treebank corpus for training\n",
    "treebank_sents = treebank.tagged_sents()\n",
    "\n",
    "# Printing a sample\n",
    "treebank_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "d73b8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"I have chosen four tags for ease:\n",
    "N:Noun\n",
    "V:Verb\n",
    "ADJ:Adjective\n",
    "ADV:Adverb\n",
    "PRON:Pronoun\n",
    "ART:Determinant\n",
    "\"\"\"\n",
    "#map the detailed tag mappings of treebank to the above 6 rest all are tagged as OTH\n",
    "tag_mapping = {\n",
    "    'NN': 'N', 'NNS': 'N', 'NNP': 'N', 'NNPS': 'N',  # Nouns\n",
    "    'VB': 'V', 'VBD': 'V', 'VBG': 'V', 'VBN': 'V', 'VBP': 'V', 'VBZ': 'V',  # Verbs\n",
    "    'JJ': 'ADJ', 'JJR': 'ADJ', 'JJS': 'ADJ',  # Adjectives\n",
    "    'RB': 'ADV', 'RBR': 'ADV', 'RBS': 'ADV',  # Adverbs\n",
    "    'PRP': 'PRON', 'PRP$': 'PRON', 'WP': 'PRON', 'WP$': 'PRON',  # Pronouns\n",
    "    'DT': 'ART',  # Determinants\n",
    "    'IN': 'PREP'  # Prepositions\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ff75bba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'N'),\n",
       " ('Vinken', 'N'),\n",
       " (',', 'OTH'),\n",
       " ('61', 'OTH'),\n",
       " ('years', 'N'),\n",
       " ('old', 'ADJ'),\n",
       " (',', 'OTH'),\n",
       " ('will', 'OTH'),\n",
       " ('join', 'V'),\n",
       " ('the', 'ART'),\n",
       " ('board', 'N'),\n",
       " ('as', 'PREP'),\n",
       " ('a', 'ART'),\n",
       " ('nonexecutive', 'ADJ'),\n",
       " ('director', 'N'),\n",
       " ('Nov.', 'N'),\n",
       " ('29', 'OTH'),\n",
       " ('.', 'OTH')]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a new dataset considering only 6 tags\n",
    "simplified_tagged_sentences = []\n",
    "# Iterate through the original Treebank tagged sentences\n",
    "for sentence in treebank.tagged_sents():\n",
    "    simplified_sentence = [(word, tag_mapping.get(tag, 'OTH')) for word, tag in sentence]\n",
    "    simplified_tagged_sentences.append(simplified_sentence)\n",
    "#Print sample:\n",
    "simplified_tagged_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "f3cbafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are our tags\n",
    "simplified_tags = ['START', 'N', 'V', 'ADJ', 'ADV','PRON','ART','PREP','OTH', 'END']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "06d167a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary\n",
    " \n",
    "tag_to_index={'START': 0, 'N': 1, 'V': 2, 'ADJ': 3, 'ADV': 4,'PRON':5,'ART':6,'PREP':7,'OTH': 8, 'END': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "3a2319d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VENUS\\AppData\\Local\\Temp\\ipykernel_37872\\3570715226.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  tag_transition_matrix/=row_sums[:,np.newaxis]\n"
     ]
    }
   ],
   "source": [
    "#Calculate tag transition probablities\n",
    "num_tags=len(simplified_tags)\n",
    "tag_transition_matrix=np.zeros((num_tags,num_tags),dtype=float)\n",
    "for sentence in simplified_tagged_sentences:\n",
    "    tags = [tag for (_, tag) in sentence]\n",
    "    \n",
    "    # Include transitions from START to the first tag and from the last tag to END\n",
    "    tags = ['START'] + tags + ['END']\n",
    "    for i in range(len(tags)-1):\n",
    "        cur_tag=tag_to_index[tags[i]]\n",
    "        next_tag=tag_to_index[tags[i+1]]\n",
    "        tag_transition_matrix[cur_tag][next_tag]+=1 #increment the count in tag transition matrice\n",
    "#Normalise for GETTING PROB\n",
    "row_sums=tag_transition_matrix.sum(axis=1)\n",
    "tag_transition_matrix/=row_sums[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "b6af7333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 2.91517629e-01, 9.45324476e-03, 4.11343894e-02,\n",
       "        4.59887583e-02, 7.35820133e-02, 2.31221257e-01, 1.29024016e-01,\n",
       "        1.78078692e-01, 0.00000000e+00],\n",
       "       [0.00000000e+00, 2.64073163e-01, 1.31014653e-01, 1.21245713e-02,\n",
       "        1.54501680e-02, 4.67662036e-03, 5.95836076e-03, 1.76672325e-01,\n",
       "        3.89337306e-01, 6.92832646e-04],\n",
       "       [0.00000000e+00, 1.18303395e-01, 1.21389570e-01, 6.97950463e-02,\n",
       "        7.42264778e-02, 3.80628314e-02, 1.42755401e-01, 9.80454222e-02,\n",
       "        3.37421856e-01, 0.00000000e+00],\n",
       "       [0.00000000e+00, 6.99390339e-01, 1.18805690e-02, 6.64373925e-02,\n",
       "        3.75175864e-03, 6.25293106e-04, 4.84602157e-03, 7.76926684e-02,\n",
       "        1.35375957e-01, 0.00000000e+00],\n",
       "       [0.00000000e+00, 2.13832275e-02, 3.51152690e-01, 1.31306382e-01,\n",
       "        8.05212162e-02, 5.67991981e-03, 5.11192783e-02, 1.24290010e-01,\n",
       "        2.34213164e-01, 3.34112930e-04],\n",
       "       [0.00000000e+00, 2.09353307e-01, 4.08111071e-01, 7.30727073e-02,\n",
       "        3.39788089e-02, 7.67263427e-03, 9.13408842e-03, 2.26525393e-02,\n",
       "        2.36024845e-01, 0.00000000e+00],\n",
       "       [0.00000000e+00, 6.80220453e-01, 2.97611758e-02, 2.18248622e-01,\n",
       "        1.29822413e-02, 1.34721372e-03, 1.22473974e-03, 9.67544397e-03,\n",
       "        4.64176363e-02, 1.22473974e-04],\n",
       "       [0.00000000e+00, 3.21903216e-01, 8.21751040e-03, 1.06523283e-01,\n",
       "        1.22755402e-02, 6.88850563e-02, 3.18555341e-01, 1.69422745e-02,\n",
       "        1.46190525e-01, 5.07253728e-04],\n",
       "       [0.00000000e+00, 1.64352410e-01, 1.62491817e-01, 3.97615684e-02,\n",
       "        2.90803845e-02, 3.79354305e-02, 6.63267064e-02, 6.32601730e-02,\n",
       "        3.02863246e-01, 1.33928264e-01],\n",
       "       [           nan,            nan,            nan,            nan,\n",
       "                   nan,            nan,            nan,            nan,\n",
       "                   nan,            nan]])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the tag TRANSITION MATRIX of our dataset (7*7 2 extra for START and END tags)\n",
    "tag_transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "f2764685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'it': 0.17391304347826086,\n",
       " 'its': 0.12130069419071976,\n",
       " 'We': 0.017172086225794667,\n",
       " 'our': 0.009499451954694921,\n",
       " 'who': 0.059554256485202774,\n",
       " 'us': 0.009499451954694921,\n",
       " 'he': 0.08403361344537816,\n",
       " 'you': 0.02557544757033248,\n",
       " 'It': 0.03653635367190355,\n",
       " 'they': 0.07672634271099744,\n",
       " 'He': 0.026671538180489587,\n",
       " 'their': 0.06613080014614542,\n",
       " 'them': 0.02557544757033248,\n",
       " 'What': 0.006941907197661673,\n",
       " 'what': 0.017902813299232736,\n",
       " 'we': 0.0175374497625137,\n",
       " 'They': 0.01936426744610888,\n",
       " 'his': 0.04420898794300329,\n",
       " 'whose': 0.005115089514066497,\n",
       " 'him': 0.005845816587504567,\n",
       " 'You': 0.00876872488125685,\n",
       " 'she': 0.028132992327365727,\n",
       " 'She': 0.0076726342710997444,\n",
       " 'themselves': 0.004384362440628425,\n",
       " 'itself': 0.0025575447570332483,\n",
       " 'I': 0.041286079649251003,\n",
       " 'Its': 0.00401899890390939,\n",
       " 'her': 0.018633540372670808,\n",
       " 'Their': 0.0010960906101571063,\n",
       " 'whom': 0.0021921812203142127,\n",
       " 'one': 0.0007307270734380709,\n",
       " 'His': 0.001826817683595177,\n",
       " 'Her': 0.0014614541468761417,\n",
       " 'your': 0.008403361344537815,\n",
       " 'Who': 0.0014614541468761417,\n",
       " 'himself': 0.003288271830471319,\n",
       " 'My': 0.0007307270734380709,\n",
       " 'me': 0.003288271830471319,\n",
       " 'my': 0.006576543660942638,\n",
       " 'herself': 0.00036536353671903543,\n",
       " 'Your': 0.0025575447570332483,\n",
       " 'IT': 0.00036536353671903543,\n",
       " 'yourself': 0.0010960906101571063}"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the emission probablities\n",
    "# Define a dictionary to store emission counts\n",
    "emission_counts = {tag: {} for tag in simplified_tags}\n",
    "\n",
    "# Count occurrences\n",
    "for sentence in simplified_tagged_sentences:\n",
    "    for word, tag in sentence:\n",
    "        #if first time encountered then initialise to 1\n",
    "        if word not in emission_counts[tag]:\n",
    "            emission_counts[tag][word] = 1\n",
    "        #increment by 1\n",
    "        else:\n",
    "            emission_counts[tag][word] += 1\n",
    "# Convert counts to probabilities by dividing by the total occurrences of each tag\n",
    "tag_counts = {tag: sum(emission_counts[tag].values()) for tag in simplified_tags}\n",
    "emission_probabilities = {tag: {word: count/tag_counts[tag] for word, count in emission_counts[tag].items()} for tag in simplified_tags}\n",
    "#View how emission probabilities dictionary look\n",
    "emission_probabilities['PRON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "70c5a37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999325\n",
      "0.9999999999999488\n",
      "1.0000000000000384\n",
      "0.999999999999995\n"
     ]
    }
   ],
   "source": [
    "#Just checking the sum of the probabilities hould be almost 1\n",
    "print(sum(emission_probabilities['N'].values()))\n",
    "print(sum(emission_probabilities['V'].values()))\n",
    "print(sum(emission_probabilities['ADJ'].values()))\n",
    "print(sum(emission_probabilities['ADV'].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "bbe1d574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['START', 'N', 'V', 'ADJ', 'ADV', 'PRON', 'ART', 'PREP', 'OTH', 'END']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "f9b47613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_tagger(sentence):\n",
    "    \n",
    "    sentence = sentence.split(' ')\n",
    "    # Number of tags\n",
    "    n = len(sentence)\n",
    "    m = len(simplified_tags)\n",
    "    \n",
    "    # This dp array will contain the probabilities, with dimensions (n, m)\n",
    "    dp = np.zeros((n, m))\n",
    "    epsilon = 0.00001 # a very small number to handle out-of-vocabulary words\n",
    "    \n",
    "    # This array will store the path to the current state\n",
    "    parent = np.zeros((n, m), dtype=int)\n",
    "    \n",
    "    # Let's initialize the first row\n",
    "    for j, tag in enumerate(simplified_tags): # go over the tags\n",
    "        if tag == 'START' or tag=='END': # if tag is 'START', continue\n",
    "            continue\n",
    "        # get the emission probabilities for the first word\n",
    "        emission = emission_probabilities[tag].get(sentence[0], epsilon)\n",
    "        dp[0][j] = tag_transition_matrix[0][j] * emission\n",
    "#     print(dp)    \n",
    "    # fill the matrix for the rest of the values\n",
    "    for i in range(1, n):\n",
    "        for j, tag in enumerate(simplified_tags):\n",
    "            if tag == 'START' or tag=='END': # if tag is 'START', continue\n",
    "                continue\n",
    "            max_prob = 0\n",
    "            best_prev_tag = -1\n",
    "            # This loop goes over each tag of the previous word\n",
    "            # and calculates the maximum probability as well as the argmax\n",
    "            for k, prev_tag in enumerate(simplified_tags):\n",
    "                if tag == 'START' or tag=='END': # if tag is 'START', continue\n",
    "                    continue\n",
    "                prob = dp[i-1][k] * tag_transition_matrix[k][j]\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    best_prev_tag = k\n",
    "            emission = emission_probabilities[tag].get(sentence[i], epsilon)\n",
    "            dp[i][j] = max_prob * emission\n",
    "            parent[i][j] = best_prev_tag\n",
    "#     print(dp)\n",
    "    best_path = []\n",
    "    # Start from the best last tag and k\n",
    "    best_last_tag = np.argmax(dp[-1])\n",
    "    best_path.append(best_last_tag)\n",
    "    \n",
    "    for i in range(n-1, 0, -1):\n",
    "        best_last_tag = parent[i][best_last_tag]\n",
    "        best_path.append(best_last_tag)\n",
    "    \n",
    "    best_path = best_path[::-1]\n",
    "    return [simplified_tags[tag_idx] for tag_idx in best_path]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "92eba264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N', 'V', 'ART', 'ADJ', 'N']"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_tagger('Ram is a nice boy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "c0d3acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "89f28d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'V', 'ART', 'ADJ', 'N']\n",
      "['PRON', 'V', 'ART', 'N', 'N']\n",
      "['PRON', 'V', 'ART', 'N', 'PREP', 'ART', 'N']\n"
     ]
    }
   ],
   "source": [
    "print(viterbi_tagger('Pierre is a nice boy'))\n",
    "print(viterbi_tagger('I like the NLP course'))\n",
    "print(viterbi_tagger('He is the director of the company'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "33cb0847",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load movie reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Shuffle the documents and split into data and labels\n",
    " \n",
    "random.shuffle(documents)\n",
    "texts = [\" \".join(words) for words, _ in documents]\n",
    "labels = [cat for _, cat in documents]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2,random_state=1)\n",
    "\n",
    "def load_glove_embeddings(path):\n",
    "    embeddings = {}\n",
    "    with open(path, 'r',encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove_embeddings = load_glove_embeddings('glove.6B.200d.txt')\n",
    "\n",
    "def text_to_embedding(text, embeddings):\n",
    "    words = text.split()\n",
    "    vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(200)  # Assuming you are using 200d GloVe embeddings\n",
    "\n",
    "X_train_embedded = np.array([text_to_embedding(text, glove_embeddings) for text in X_train])\n",
    "X_test_embedded = np.array([text_to_embedding(text, glove_embeddings) for text in X_test])\n",
    "\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train_embedded, y_train)\n",
    "\n",
    "y_test_pred = clf.predict(X_test_embedded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "7ac5c3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.64      0.71      0.67       193\n",
      "         pos       0.70      0.63      0.66       207\n",
      "\n",
      "    accuracy                           0.67       400\n",
      "   macro avg       0.67      0.67      0.67       400\n",
      "weighted avg       0.67      0.67      0.67       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "4bc4f90d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# POS Tagging using Viterbi\n",
    "def tag_fetch(sentence):\n",
    "    return viterbi_tagger(sentence)\n",
    "\n",
    "#tag weights We assign weights only to get the adjectives and adverbs\n",
    "#as they majorly decribe the sentiment\n",
    "tag_weights = {\n",
    "    'N': 0, 'V': 0, 'ADJ': 1.0, 'ADV': 1.0,\n",
    "    'PRON':0, 'ART':0, 'PREP':0.0, 'OTH':0.0\n",
    "}\n",
    "\n",
    "# Dividing the Text into Sections of 30 words each\n",
    "def section_text(data,section_size=30):\n",
    "    items = re.split(r'[.\\t ]+', data)\n",
    "    sections = [' '.join(items[i:i+section_size]) for i in range(0, len(items), section_size)]\n",
    "    return sections\n",
    "\n",
    "# Convert Text to Weighted Vector\n",
    "#This is the crux here we multiply the adjective and adverb embeddings by 1 and rest by zero\n",
    "def text_to_vector(data, embedding_model, tag_weights):\n",
    "    items = data.split()\n",
    "    tags = tag_fetch(data)\n",
    "    vectors_list = []\n",
    "    \n",
    "    for item, tag in zip(items, tags):\n",
    "        wt = tag_weights.get(tag, 1.0)\n",
    "        if item in embedding_model:\n",
    "            vectors_list.append(embedding_model[item] * wt)\n",
    "        else:\n",
    "            vectors_list.append(np.zeros(200))\n",
    "\n",
    "    if vectors_list:\n",
    "        return np.mean(vectors_list, axis=0)\n",
    "    else:\n",
    "        return np.zeros(200)\n",
    "\n",
    "# Vector Generation for Sections\n",
    "#Take the mean of all Glove embeddings of all the words as the representation of the text\n",
    "def section_to_vector(data, embedding_model, tag_weights,section_size=30):\n",
    "    sections = section_text(data, section_size)\n",
    "    section_vectors = [text_to_vector(section, embedding_model, tag_weights) for section in sections]\n",
    "    return np.mean(section_vectors, axis=0)\n",
    "\n",
    "# Load Embedding Model\n",
    "#Create a dictionry of word vs Glove Embeddings\n",
    "def fetch_embeddings(filepath):\n",
    "    embedding_dict = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vec = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_dict[word] = vec\n",
    "    return embedding_dict\n",
    "\n",
    "embeddings_dict = fetch_embeddings('glove.6B.200d.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "f8c7bdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 69.50%\n"
     ]
    }
   ],
   "source": [
    "# Convert All Data to Weighted Vectors\n",
    "X_train_vec_vt = np.array([section_to_vector(text, embeddings_dict, tag_weights) for text in X_train])\n",
    "X_test_vec_vt = np.array([section_to_vector(text, embeddings_dict, tag_weights) for text in X_test])\n",
    "\n",
    "# Train and Evaluate the Model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_vec_vt, y_train)\n",
    "predicted_labels_viterbi = model.predict(X_test_vec_vt)\n",
    "\n",
    "model_accuracy = accuracy_score(y_test, predicted_labels_viterbi)\n",
    "print(f\"Model Accuracy: {model_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "26f56d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.68      0.70      0.69       193\n",
      "         pos       0.71      0.69      0.70       207\n",
      "\n",
      "    accuracy                           0.69       400\n",
      "   macro avg       0.70      0.70      0.69       400\n",
      "weighted avg       0.70      0.69      0.70       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predicted_labels_viterbi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
